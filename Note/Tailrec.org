#+Title:  Tail recursion optimization
#+Author: Comcx

#+SETUPFILE: theme-readtheorg-local.setup


Many algorithms can be expressed concisely and elegantly in functional languages with recursions.
However, not all languages support automatic optimizations respect to functional algorithms and,
some languages(such as /Scala/), although support basic =@tailrec= optimizations by code transformation,
they still can not do some agressive tailrec optimizations such as =CPS tailrec=.

Therefore, we need to transform programs by hand. With this technique, we can use functional style
fearlessly for that we can translate our code into traditional imperative ways.

In studying this, *Scala* can play an important role in tranlating our programs since /Scala/ support
many different programming strategies. To make our method more impressive, we will finally try to translate
programs into effective *C++* codes.

We follow this program route:
#+BEGIN_SRC 
Haskell ==> Scala ==> C++
#+END_SRC

(This is just a try, still not sure if I can make it :-)


** Tailrec strategies

*** Linear recursion
**** Formalization
To make things simple but not stupid, we firstly only focus on recursions whose
last expression only contains one recursion
(which means we do not discuss tree recursion right now).

#+BEGIN_SRC haskell
rec 1 = some value
rec x = f x (rec (h x))
#+END_SRC

Now we unfold this recursion:
#+BEGIN_SRC haskell
rec x = f x $ f (h x) $ f (h $ h x) $ f (h $ h $ h x) $ ... $ f (h $ h $ ... $ h $ x) $ (some value)
#+END_SRC

Hummmmm, it looks even more complex...
I now have two guesses on how to do tailrec for codes like this:
1) Build from bottom or reverse order
2) Do /CPS/ transformation

Let's try the first one first.
To build from bottom, we need to know the most important parameter: =(h $ h $ ... $ x)=.
To know this parameter, we need to know how many times the function =h= was
applied to =x=. We can find some clues from the terminating pattern of =rec=.

**** Reverse order
- Our current problem:

  #+BEGIN_SRC 
  We have a function (h :: a -> a) which do transformations of x and
  we need to figure out that how many times h applied to x in order to
  reach the value '1'(some constant or pattern) in the terminating definiton of rec.
  #+END_SRC

***** *If h is reversible(noted h', i.e. h . h' = id)*

    we can build from bottom!
    Notice that we need to pass our value of each layer to upper layer,
    so we need one more parameter to record our current result.
#+BEGIN_SRC haskell
reverseBuild f h x res = f x res
reverseBuild f h y res = reverseBuild f h (h' y) (f y res)
#+END_SRC
    * use foldl, foldr?

***** *else h is not reversible:(*

    well, which means we can't get results from bottom, we have to start from
    head since we can not calculate (h $ h $ ... $ x). Maybe we need another strategy.

***** *To change order... How about =Monoid=?*

    Suppose we want to get the sum of a list:
#+BEGIN_SRC haskell
sum [] = 0
sum (x:xs) = x + sum xs
#+END_SRC
    where, =(+)= becomes the function =f=.

    Since (a + b) + c === a + (b + c),
    we can reverse the calculation order
#+BEGIN_SRC haskell
a + (b + (c + d))
#+END_SRC
    to
#+BEGIN_SRC haskell
((a + b) + c) + d
#+END_SRC
    It's just like to say that, we can change the calculation of
#+BEGIN_SRC haskell
x (h x) (h $ h x) ... (h $ h $ ... $ h x)
#+END_SRC
    with =f=.

    Let's try to implement this transformation with our general code:
#+BEGIN_SRC haskell
reverseOrder f h 1 res = res
reverseOrder f h x res = reverseOrder f h (h x) (f x (h x))
#+END_SRC
    This time we don't need =h'=!

***** *Being inspired by monoid, maybe be we can again generize our 'monoid approach' a little bit*

    This time instead of requiring f to be satisfiled by monoid law,
    we ourselves try to find a function =g= which satisfy the following law:
#+BEGIN_SRC haskell
g (g a b) c = f a (f b c)
#+END_SRC
    Now we can use our trick again:
#+BEGIN_SRC haskell
reverseOrder' g h 1 res = res
reverseOrder' g h x res = reverseOrder' g h (h x) (g x (h x))
#+END_SRC

    Even more general(although harder...), we can generalize function =h= too.
    Suppose we have our special function =g= and a new h function =p=.
    
    We have general rule:
#+BEGIN_SRC haskell
g (g x (p x)) (p $ p x) = f x (f (h x) (h $ h x))
or(with extra start x0)
g (g (g a x) (p x)) (p $ p x) = f x (f (h x) (h $ (h x)))
#+END_SRC
    What's more? We may Just add a new start data x0(just like foldl :-)

    The final version with extra x0(a is x0 at first):
#+BEGIN_SRC haskell
reverseOrder'' g p a 1 res = res
reverseOrder'' g p a x res = reverseOrder'' g p (p x) (g a x)
#+END_SRC
   
***** *Well, if we're not lucky.. we can't find a reverse h' nor monoid structure...*
     
    Two options:
    1) We still calculate from the beginning, but we pass the result as a =List= of (h $ ..x)s.
        But this is not efficient and ugly...
    2) Maybe it's time for our /CPS/ approach...

Time for the second idea!
**** CPS trasformation

CPS is a tool to convert recursive code to *tail call*.(note that this is different from tailrec).
Recall our =rec= function:
#+BEGIN_SRC haskell
rec 1 = some value
rec x = f x (rec (h x))
#+END_SRC

to /CPS/ style =>
#+BEGIN_SRC haskell
rec 1 c = c value
rec x c = rec (h x) (c . (f x))
#+END_SRC
It looks like we are just building our unfolded function chain.
Instead of accumulating results and values, we accumulate calculations(functions).

In fact, CPS can be used to convert any recursive function to tail-call form.
With CPS, we can build our executing stack from heap.

**** Summary
We have discussed several optimizations may be applied to list recursions.
Let's recap!

***** Tailrec
****** Calculating from bottom if reversible
****** Reverse calculation order
- By Monoid structure
- By special function g with specific rules
****** CPS transformation 

***** Dynamic Programming

*** Tree recursion
Well, it seems that linear tailrec is not easy now, how about tree tailrec?
**** Two-branch tree recursion
***** Formalization
#+BEGIN_SRC haskell
rec 1 = some value
rec x = f x (rec (g x)) (rec (h x))
#+END_SRC
If we unfold thie expression, this expression can be a very large binary-tree.
#+BEGIN_SRC 
                               f x
                     /                     \
          f $ g x                                 f $ h x
        /         \                            /           \
f $ g x $ g x     f $ g x $ h x    f $ h x $ g x            f $ h x $ h x

...
#+END_SRC
It looks like we are traversing all possible permutations of function =g= and =h= compositions

***** A simple try(baby step)

A very simple idea is that if we can tailrec =rec=, then we want to cache result of last step:
#+BEGIN_SRC haskell
rec' x a 
  = p (rec x) a
  = p (f x (rec (g x)) (rec (h x))) a
#+END_SRC
What if...
#+BEGIN_SRC haskell
p (f x (rec (g x)) (rec (h x))) a 
  = (rec (g x)) `f x` (rec (h x)) `p` a
  if f x a b == a `q` x `q` b
  = (rec (g x)) `q` x `q` (rec (h x)) `p` a
  = (rec (g x)) `q` x `q` (p (rec (h x)) a)
  = (rec (g x)) `q` x `q` (rec' (h x) a)
  if p == q
  = (rec (g x)) `p` x `p` (rec' (h x) a)
  if (a `p` b) `p` c == a `p` (b `p` c)
  = (rec (g x)) `p` (x `p` (rec' (h x) a))
  = p (rec (g x)) (p x (rec' (h x) a))
  = rec' (g x) (p x (rec' (h x) a))
#+END_SRC
Wait! we have made too many assumptions!
Let's list them all:
#+BEGIN_SRC
1. rec' x a = p (rec x) a
2. f x a b = a `p` x `p` b
3. (a `p` x) `p` b = a `p` (x `p` b)
#+END_SRC
In words, we can find a function =p=, which 
1) can be expressed as a composition of
   raw tree recursion version and a accumulation value
2) Function =f= can in raw tree recursion can be decomposed as compositions of function =p=.
3) Function =p= satisfy _Monoid law_.

These constraints looks really strict and ugly...
The first condition is easy to reach since we can simply build a function =p=.
The second is hard. we need to decompose =f= to our built function =p=.
Well, I want to have a try.

Can this strategy solve =fib= problem?
#+BEGIN_SRC haskell
fib 0 = 0
fib 1 = 1
fib n = fib (n-1) + fib (n-2)
#+END_SRC
#+BEGIN_SRC haskell
rec x = f x (rec (g x)) (rec (h x))

rec = fib
f x a b = a + b
g = (minus 1)
h = (minus 2)
#+END_SRC
OK, Let's build function =p=.
#+BEGIN_SRC haskell
fib' n a 
  = fib n + a
  = fib (n-1) + fib (n-2) + a
  = fib (n-1) + fib' (n-2) a
  = fib' (n-1) (fib' (n-2) a)
#+END_SRC
Therefore,
#+BEGIN_SRC haskell
fib' 0 a = a
fib' 1 a = 1 + a
fib' n a = fib' (n-1) (fib' (n-2) a)
#+END_SRC
We succeeded!
However, this is just our first baby step, this is just =tail call=, not =tailrec=.

***** To the real tailrec!
We all know that =fibonacci= problem can be expressed as loops which is real =tailrec=.
Many tree-recursions can be optimized as tailrecs. Clearly that we need to step further.

****** What have we done?
Let's just use our =fibonacci= example.
Although we have successfully defined our _tail call_ function, we can not see what's happening inside.
Let's unfold function =fib'=.

#+BEGIN_SRC haskell
fib' n 0
  = fib' (n-1) (fib' (n-2) 0)
  = fib' (n-2) (fib' (n-3) $ fib' (n-2) 0)
  = fib' (n-3) (fib' (n-4) $ fib' (n-3) $ fib' (n-2) 0)
  = fib' (n-4) (fib' (n-5) $ fib' (n-4) $ fib' (n-3) $ fib' (n-2) 0)
  ...
#+END_SRC
We have generated a foldable list!
However, if we want to get the result inside, we need to handle the most inner term:
#+BEGIN_SRC haskell
fib' (n-4) (fib' (n-5) $ fib' (n-4) $ fib' (n-3) $ fib' (n-2) 0)
  = fib' (n-4) (fib' (n-5) $ fib' (n-4) $ fib' (n-3) $ fib' (n-3) $ fib' (n-4) 0)
  = fib' (n-4) (fib' (n-5) $ fib' (n-4) $ fib' (n-3) $ fib' (n-3) $ fib' (n-5) $ fib' (n-6) 0)
  = fib' (n-4) (fib' (n-5) $ fib' (n-4) $ fib' (n-3) $ fib' (n-3) $ fib' (n-5) $ fib' (n-7) $ fib' (n-8) 0)
  ...
#+END_SRC
OK, enough!
Whenever we get an inner result, we need to re-unfold our whole expression!
This looks no difference between tree-recursion version!
But this =tail call= version is in fact faster than raw tree-recursion in /Haskell(because of Haskell's laziness, implied CPS)/, 
and we can see stack-accumulating-like progress directly!

****** Where to go next?
So far, we have not figured out any useful technique at all...
I plan to try to understand _real tailrec_ of =fib= directly.

***** Code transformation again
The =fib= function again!
#+BEGIN_SRC haskell
-- Step 1 ==> A first attempt is to add fib(n + 1) to both sides of the equation:
fib (n + 1) + fib (n + 2) = fib n + 2 * fib (n + 1)

-- Step 2 ==> The two sides of the equation look much more alike, 
-- but there is still an essential difference, which is the coefficient of the second term of each side. 
-- On the left side of the equation, it is 1 and, on the right, it is 2. 
-- To remedy this, we can introduce a variable b:
fib (n + 1) + b * fib (n + 2) = b * fib n + (b + 1) * fib (n + 1)

-- Step3 ==> We notice that the coefficients of the first term are not the same (1 on the left and b on the right), 
-- so we introduce a variable a:
a * fib (n + 1) + b * fib (n + 2) = b * fib n + (a + b) * fib (n + 1)

-- Step 4 ==> Now the two sides have the same form (call it F), which we can define as:
F a b n = a * fib n + b * fib (n + 1)

-- Then we have
F a b (n + 1) = F b (a + b) n

-- and
F a b 0 = a * fib 0 + b * fib 1 = a + b

-- Step 5 ==> Finally, by definition of F:
fib n = F 1 0 n
#+END_SRC
Cool! Are there any patterns inside?
This time in stead of use an accumulating variable, we try to change our whole function states!

But how to get new state definitions? It seems that the strategy above is trying to combine two states into one.
#+BEGIN_SRC haskell
if rec' a b x == f' a b (rec (g' x)) (rec (h' x)) 
if f' a b (rec (g' x)) (rec (h' x)) == f' (p a b) (q a b) (rec (g_ $ g' x)) (rec (h_ $ h' x))
rec' a b x = rec' (p a b) (q a b) (t x)
if exists a0, b0 satisfy rec' a0 b0 x = f' a0 b0 (rec (g' x)) (rec (h' x)) == rec x
rec x = rec' a0 b0 x
#+END_SRC
It looks that the hardest one is _how to get function t_.

If we want to find function =t=, we need to have this more constraint:
#+BEGIN_SRC haskell
if g_ (g' x) = g' (g_' x) && h_ (h' x) = h' (h_' x)
   h' x = hg (g' x)
then we have 1-to-1 relation:
  x <--> ((g' x), (hg $ g' x))
  (g' (g_' x), hg (g' $ g_' x)) <--> g_' x 

t = g_'
#+END_SRC
Sadly we have to add one relation function =hg= to get function =t=.

Let's sort what we have found:
#+BEGIN_SRC haskell
if rec' a b x == f' a b (rec $ g' x) (rec $ h' x)
   f' a b (rec $ g' x) (rec $ h' x) == f' (p a b) (q a b) (rec $ (g_ $ g' x)) (rec $ (h_ $ h' x))
   exists a0, b0 satisfy rec' a0 b0 x = f' a0 b0 (rec (g' x)) (rec (h' x)) == rec x
   g_ (g' x) = g' (g_' x) && h_ (h' x) = h' (h_' x)
   h' x = hg (g' x)

t = g_'
rec' a b x = rec' (p a b) (q a b) (t x)
rec x = rec' a0 b0 x
#+END_SRC
In words, we need to find a function =f'=, which can be resolved by _rec'_ and resolved to recussion.
And there needs to exist one relation =hg= between =h'= and =g'=.

Let's firstly make function =f'= linear.
#+BEGIN_SRC haskell
if f' a b rgx rhx = a*rgx + b*rhx

rec' a b x = a * rec (g' x) + b * rec (h' x)
a * rec (g' x) + b * rec (h' x) = (p a b) * rec (g_ $ g' x) + (q a b) * rec (h_ $ h' x)
g_ (g' x) = g' (g_' x) && h_ (h' x) = h' (h_' x)
h' x = hg (g' x)
#+END_SRC
Not enough, make function =p= and =q= linear too:
#+BEGIN_SRC haskell
if f' a b rgx rhx = a*rgx + b*rhx
   p a b = mp * a + np * b + kp
   q a b = mq * a + nq * b + kq

a * rec (g' x) + b * rec (h' x) 
  = (mp*a + np*b + kp) * rec (g_ $ g' x) + (mq*a + nq*b + kq) * rec (h_ $ h' x)

#+END_SRC




**** N-branch tree recursion

** Examples
*** Selection sort
*** Fibonacci
*** Permutations










