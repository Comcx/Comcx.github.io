#+Title:  Sorting Algorithms: revisited
#+Author: Comcx
#+Date:   <2020-01-28 周二>


We're going to revisite some main-stream sorting algorithms and
try to implement them in a more efficient way.

To make things simple, we choose _Haskell_ to describe basic recursions and
ideas, then we directly translate haskell codes into _Scala_ since Scala serves
a great role between purely functional way and more traditional ways.

Finally, we translate Scala codes into _C++_ (some even C:).
If you have some better ideas or implementations, please email _comcx@outlook.com_.

- Our outline
#+BEGIN_SRC 
Haskell recursion                    ==> 
Scala equivalent                     ==> 
Scala more traditional and efficient ==>
C++ implementation                  (==> C)
#+END_SRC
Let's go!

** Overview
*** Comparison-based sorting
A Haskell blueprint of comparison-based sorting is
#+BEGIN_SRC haskell

sort :: Ord a => [a] -> [a]
#+END_SRC

Nearly all comparison-based algorithms rely on the following principles:
1. Split the list =L= into two lists =L1= and =L2=, sort them resursively.
2. Join the result together.

From these principles, we have the following table:
| Type                 | Singleton      | Equal size |
|----------------------+----------------+------------|
| hard split/easy join | Selection sort | Quick sort |
|                      | Heap sort      |            |
| easy split/hard join | Insertion sort | Merge sort |
|                      | Tree sort      |            |
where =Singleton/Equal size= refers the length of =L1= compared to =L2=.

- Visualization
#+BEGIN_SRC 
                           List L
                             ||
                             \/
                           split L 
                    /                   \
          sort L1                           sort L2
                   \                    /
                         join L1 L2
                             ||
                             \/
                        Sorted List L
#+END_SRC
Ok, now let's try to implement these algorithms in =Haskell=.

*** Representation-based sorting

** Algorithms in /Haskell/ 
*** Selection sort
- *A naive implementation*
#+BEGIN_SRC haskell

ssort :: Ord a => [a] -> [a]
ssort [] = []
ssort xs = m : ssort $ delete m xs
  where m = minimum xs
#+END_SRC
Well although this is not efficient, 
this is quite clear and easy to understand.

A more efficient implemetation of Haskell maybe:
#+BEGIN_SRC haskell

split :: Ord a => [a] -> a -> [a] -> [a]
split [] m r     = m : ssort' r
split (x:xs) m r | x < m     = split xs x (m:r)
                 | otherwise = split xs m (x:r)

ssort' []     = []
ssort' (x:xs) = split xs x []
#+END_SRC
or just make the naive one /tailrec/.

However, this way it's not as clear as the naive one.

*** Insertion sort
This algorithm is ubiquitous in our daily lives.
#+BEGIN_SRC haskell

insert k [] = [k]
insert k l@(x:xs)
  | k <= x    = k : l
  | otherwise = x : (insert k xs)

isort = foldr insert []
#+END_SRC

*** Quick sort
Quick sort well, is famous...
many people was attracted by the naive implementation in haskell...
#+BEGIN_SRC haskell

qsort :: ord a => [a] -> [a]
qsort [] = []
qsort (x:xs) = qsort lower ++ [x] ++ qsort upper
  where lower = [e | e <- xs, e <= x]
        upper = [e | e <- xs, e >  x]
#+END_SRC

Well, just improve a little bit(tailrec):
#+BEGIN_SRC haskell

qsort' [] s     = s
qsort' (x:xs) s = qsort' lower (x : qsort' upper s)
  where lower = [e | e <- xs, e <= x]
        upper = [e | e <- xs, e >  x]
#+END_SRC

We can also split the list in fewer passes, but I think it's not necessary.

*** Merge sort
Merge sort is a well balanced algorithm.

Firstly, we need to know how to merge two ordered lists.
#+BEGIN_SRC haskell

merge :: Ord a => [a] -> [a] -> [a]
merge [] b = b
merge a [] = a
merge a@(x:xs) b@(y:ys)
  | x <= y    = x : merge xs b
  | otherwise = y : merge a ys
#+END_SRC

Now all we need is to split list in a balanced way.
#+BEGIN_SRC haskell

msort :: Ord a => [a] -> [a]
msort []  = []
msort [x] = [x]
msort xs  = merge (msort as) (msort bs)
  where as = take k xs
        bs = drop k xs
        k  = (length xs) `div` 2
#+END_SRC

*** Heap sort
#+BEGIN_SRC 
UNDERWORK!
#+END_SRC


** Algorithms in /Scala/ 
In this part, we firstly translate haskell code directly into Scala
and then try to improve codes to a more traditional program.

At first, we will try to be as functional as possible, every variable should
not be changed. However, scala also support traditional variables which can
be easily changed. We will implement both versions.

*** Selection sort
**** Naive translation in Scala
#+BEGIN_SRC scala

object SelectionSort {

  def remove[A](x: A, list: List[A]) = list diff List(x)

  //Functional version 1
  def ssort1[A <% Ordered[A]](xs: List[A]): List[A] = xs match {
    case Nil => Nil
    case _   => xs.min :: ssort1(xs remove xs.min)
  }
}
#+END_SRC
Well, before we get into more traditional ways of Selection sort,
Let's try our best to optimize our functional version.

We mainly apply three optimizations: *tailrec* , *one-pass* and *Dynamic programming*.

**** Tailrec
The *tailrec* optimization is efficient and crucial for our further
traditional implementations. However, how do we do tailrecs?

- *Simple list tailrec*

To make things simple and stupid, we firstly only focus on recursions whose
last expression only contains one recursion
(which means we do not discuss tree recursion right now).

#+BEGIN_SRC 
rec 1 = some value
rec x = f x (rec (h x))
#+END_SRC

Now we unfold this recursion:
#+BEGIN_SRC 
rec x = f x $ f (h x) $ f (h $ h x) $ f (h $ h $ h x) $ ... $ f (h $ h $ ... $ h $ x) $ (some value)
#+END_SRC

Hummmmm, it looks even more complex...
I now have two guesses on how to do tailrec for codes like this:
1) Build from bottom or reverse order
2) Do /CPS/ transformation

Let's try the first one first.
To build from bottom, we need to know the most important parameter: =(h $ h $ ... $ x)=.
To know this parameter, we need to know how many times the function =h= was
applied to =x=. We can find some clues from the terminating pattern of =rec=.


***** Reverse order
- Our current problem:

  #+BEGIN_SRC 
  We have a function (h :: a -> a) which do transformations of x and
  we need to figure out that how many times h applied to x in order to
  reach the value '1'(some constant or pattern) in the terminating definiton of rec.
  #+END_SRC

  + If h is reversible(noted h', i.e. h . h' = id)

    we can build from bottom!
    Notice that we need to pass our value of each layer to upper layer,
    so we need one more parameter to record our current result.
    #+BEGIN_SRC 
    reverseBuild f h x res = f x res
    reverseBuild f h y res = reverseBuild f h (h' y) (f y res)
    #+END_SRC
    * use foldl, foldr?


  + else h is not reversible:(

    well, which means we can't get results from bottom, we have to start from
    head since we can not calculate (h $ h $ ... $ x). Maybe we need another strategy.


  + To change order... How about =Monoid=?

    Suppose we want to get the sum of a list:
    #+BEGIN_SRC 
    sum [] = 0
    sum (x:xs) = x + sum xs
    #+END_SRC
    where, =(+)= becomes the function =f=.

    Since (a + b) + c === a + (b + c),
    we can reverse the calculation order
    #+BEGIN_SRC 
    a + (b + (c + d))
    #+END_SRC
    to
    #+BEGIN_SRC 
    ((a + b) + c) + d
    #+END_SRC
    It's just like to say that, we can change the calculation of
    #+BEGIN_SRC 
    x (h x) (h $ h x) ... (h $ h $ ... $ h x)
    #+END_SRC
    with =f=.

    Let's try to implement this transformation with our general code:
    #+BEGIN_SRC 
    reverseOrder f h 1 res = res
    reverseOrder f h x res = reverseOrder f h (h x) (f x (h x))
    #+END_SRC
    This time we don't need =h'=!


  + Being inspired by monoid, maybe be we can again generize our 'monoid approach' a little bit

    This time instead of requiring f to be satisfiled by monoid law,
    we ourselves try to find a function =g= which satisfy the following law:
    #+BEGIN_SRC 
    g (g a b) c = f a (f b c)
    #+END_SRC
    Now we can use our trick again:
    #+BEGIN_SRC 
    reverseOrder' g h 1 res = res
    reverseOrder' g h x res = reverseOrder' g h (g x) (g x (h x))
    #+END_SRC

   
  + Well, if we're not lucky.. we can't find a reverse h' nor monoid structure...
     
    Two options:
    1) We still calculate from the beginning, but we pass the result as a =List= of (h $ ..x)s.
        But this is not efficient and ugly...
    2) Maybe it's time for our /CPS/ approach...


Time for the second idea!
***** CPS trasformation

CPS is a tool to convert recursive code to *tail call*.(note that this is different from tailrec).
Recall our =rec= function:
#+BEGIN_SRC 
rec 1 = some value
rec x = f x (rec (h x))
#+END_SRC

to /CPS/ style =>
#+BEGIN_SRC 
rec 1 c = c value
rec x c = rec (h x) (c . (f x))
#+END_SRC
It looks like we are just building our unfolded function chain.
Instead of accumulating results and values, we accumulate calculations(functions).

In fact, CPS can be used to convert any recursive function to tail-call form.
With CPS, we can build our executing stack from heap.

**** One-pass
Well, notice that we need to traverse the list two times:
1) find minimum and 
2) recursion.

We can optimize by /one-pass/ just like the version in /Haskell/.
But this is not so necessary for us to translate codes to traditinal codes.

**** Dynamic Programming
Well, for recursions like list recursions, dynamic programming is not
quite necessary since we usually don't have too many overlapping sub-problems.

However, just in case, I think that if necessary, we can use lazy evaluation
and stream to do dynamic programming elegantly.

For more dynamic programming techniques, we will discuss in later parts(tree recursions).

**** Optimizations summary
We have discussed several optimizations may be applied to list recursions.
Let's recap!

***** Tailrec
****** Reverse calculation order
- By reversible function or special function
- By Monoid structure
****** CPS transformation 

***** Dynamic Programming
To avoid unnecessary calculation

**** Applying our optimizations 
***** Choose optimizing strategy
Now we try to optimize our _selection sort_.

- What is function =h= ?

  #+BEGIN_SRC 
  h xs = delete xs (minimum xs)
  #+END_SRC

- What is function =f= ?

  #+BEGIN_SRC 
  f x xs = (minimum x) : xs
  #+END_SRC

- Is function =h= reversible?

  well, obviously we can't. Since we can't recover a list from [].

- Can we find a monoid structure?

  No again. 
  #+BEGIN_SRC 
  f a (f b c) =/= f (f a b) c
  #+END_SRC
  This can be verified easily.

- Can we find a special function =g= to reverse calculation order?

  Recall that function =g= must satisfy:
  #+BEGIN_SRC 
  g (g a b) c = f a (f b c)
  #+END_SRC  
  This looks a little hard though...
  
  A possible solution:
  #+BEGIN_SRC 
  g a xs = a ++ minimum xs
  #+END_SRC

- CPS style?
  Yes.

*****  Let's do it!
#+BEGIN_SRC scala

object SelectionSort {

  /**
   * calculates the minimum of a list and 
   * returns the list with that element removed
   */
  def minimum[A <% Ordered[A]](xs: List[A]): List[A] =
    (List(xs.head) /: xs.tail) {
      (ys, x) =>
        if(x < ys.head) x :: ys
        else ys.head :: x :: ys.tail
  }
  
  /**
   * Functional version 2(tailrec version)
   */
  def ssort2[A <% Ordered[A]](xs: List[A]): List[A] = {
    @tailrec
    def ssort2T[A <% Ordered[A]](xs: List[A], a: List[A]): List[A] = 
      if(xs.isEmpty) a 
      else ssort2T(minimum(xs), a ::: List(xs.min))
      
    ssort2T(xs, Nil) 
  }

  /**
   * CPS version
   */
  def ssort3[A <% Ordered[A]](xs: List[A]): List[A] = {
    
    @tailrec
    def ssort3T[A <% Ordered[A]](xs: List[A], a: List[A] => List[A]) = 
      if(xs.isEmpty) a(xs)
      else ssort3T(minimum(xs), (ys: List[A]) => xs.min :: ys)
  }
  

}
#+END_SRC

Realize that we need some tools to test the efficiency of our codes.


*** Insertion sort

** Algorithms in /C++/ 
*** Selection sort
Since we have get tailrec form, we can easily convert the algorithm into loops.




